# LlamaIndex의 컴포넌트란?

1유닛의 집사 에이전트 Alfred를 기억하시나요?
Alfred가 효과적으로 돕기 위해서는 우리의 요청을 이해하고, **관련 정보를 준비·검색·활용**해야 합니다.
이때 LlamaIndex의 컴포넌트가 필요합니다.

LlamaIndex에는 다양한 컴포넌트가 있지만, **여기서는 `QueryEngine` 컴포넌트에 집중**합니다.
이유는? 에이전트의 RAG(Retrieval-Augmented Generation) 도구로 쓸 수 있기 때문입니다.

RAG란? LLM은 방대한 데이터로 학습되어 일반 지식을 갖지만, 최신/특정 데이터는 알지 못합니다.
RAG는 내 데이터에서 관련 정보를 찾아 LLM에 제공해 답변의 정확성을 높입니다.

![RAG](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/rag.png)

Alfred의 예시를 생각해봅시다:

1. 저녁 파티 계획을 부탁
2. Alfred는 캘린더, 식단, 과거 메뉴를 확인
3. `QueryEngine`이 이 정보를 찾아 파티 계획에 활용

즉, `QueryEngine`은 LlamaIndex에서 **에이전틱 RAG 워크플로우의 핵심 컴포넌트**입니다.
Alfred가 집안 정보를 검색하듯, 에이전트도 관련 데이터를 찾고 이해할 수 있어야 합니다.
`QueryEngine`이 바로 이 역할을 합니다.

이제 컴포넌트를 더 깊이 살펴보고, **컴포넌트를 조합해 RAG 파이프라인을 만드는 방법**을 알아봅시다.

## 컴포넌트로 RAG 파이프라인 만들기

<Tip>
<a href="https://huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/components.ipynb" target="_blank">이 노트북</a>의 코드를 따라가며 Google Colab에서 실습할 수 있습니다.
</Tip>

RAG에는 다섯 가지 주요 단계가 있습니다. 대부분의 대규모 애플리케이션도 이 단계를 따릅니다:

1. **로딩**: 텍스트, PDF, 웹, DB, API 등에서 데이터를 가져옵니다. LlamaHub에는 수백 개의 통합 로더가 있습니다.
2. **인덱싱**: 데이터를 쿼리할 수 있도록 인덱스(주로 벡터 임베딩)를 만듭니다. 의미 기반 검색이 가능해집니다.
3. **저장**: 인덱스와 메타데이터를 저장해 재사용합니다.
4. **쿼리**: 다양한 쿼리 전략(서브쿼리, 멀티스텝, 하이브리드 등)으로 데이터를 검색합니다.
5. **평가**: 응답의 정확성, 신뢰성, 속도를 평가해 워크플로우를 개선합니다.

이제 각 단계를 컴포넌트로 구현하는 방법을 봅시다.

### 문서 로딩 및 임베딩

LlamaIndex는 내 데이터 위에서 동작합니다. **먼저 데이터를 로드해야 합니다.**
데이터 로드 방법은 세 가지:

1. `SimpleDirectoryReader`: 로컬 폴더의 다양한 파일을 불러오는 내장 로더
2. `LlamaParse`: 공식 PDF 파서(유료 API)
3. `LlamaHub`: 모든 소스에서 데이터를 가져오는 수백 개의 로더 레지스트리

<Tip><a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/">LlamaHub</a> 로더와 <a href="https://github.com/run-llama/llama_cloud_services/blob/main/parse.md">LlamaParse</a> 파서 참고</Tip>

가장 간단한 방법은 `SimpleDirectoryReader`로 폴더에서 데이터를 불러오는 것입니다.

```python
from llama_index.core import SimpleDirectoryReader

reader = SimpleDirectoryReader(input_dir="path/to/directory")
documents = reader.load_data()
```

문서를 불러온 뒤, 더 작은 `Node` 단위로 쪼갭니다.
`Node`는 원본 문서의 일부 텍스트로, AI가 다루기 쉽고 원본 참조도 가능합니다.

`IngestionPipeline`은 두 가지 변환으로 노드를 만듭니다:
1. `SentenceSplitter`: 문서를 자연스러운 문장 단위로 쪼갬
2. `HuggingFaceEmbedding`: 각 조각을 의미 임베딩(벡터)으로 변환

이 과정으로 검색/분석에 최적화된 구조가 됩니다.

```python
from llama_index.core import Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_overlap=0),
        HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5"),
    ]
)

nodes = await pipeline.arun(documents=[Document.example()])
```

### 문서 저장 및 인덱싱

`Node`를 만들었으면, 검색을 위해 인덱싱해야 합니다. 인덱싱 전에는 데이터를 저장할 공간이 필요합니다.

IngestionPipeline을 쓴다면, 벡터 스토어를 파이프라인에 바로 연결해 데이터를 저장할 수 있습니다.
여기서는 `Chroma`를 사용합니다.

<details>
<summary>ChromaDB 설치</summary>
<a href="./llama-hub">LlamaHub</a>에서 소개한 대로, ChromaDB 벡터 스토어는 다음 명령으로 설치합니다:

```bash
pip install llama-index-vector-stores-chroma
```
</details>

```python
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore

db = chromadb.PersistentClient(path="./alfred_chroma_db")
chroma_collection = db.get_or_create_collection("alfred")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5"),
    ],
    vector_store=vector_store,
)
```

<Tip>다양한 벡터 스토어는 <a href="https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/">LlamaIndex 문서</a> 참고</Tip>

임베딩된 쿼리와 노드를 같은 벡터 공간에 매핑해 관련 정보를 찾습니다.
`VectorStoreIndex`가 이 과정을 자동으로 처리합니다.

```python
from llama_index.core import VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)
```

모든 정보는 `ChromaVectorStore`와 지정한 경로에 자동 저장됩니다.

이제 인덱스를 쉽게 저장/불러올 수 있으니, 다양한 쿼리 방법을 살펴봅시다.

### 프롬프트/LLM으로 VectorStoreIndex 쿼리하기

인덱스를 쿼리하려면 쿼리 인터페이스로 변환해야 합니다. 주요 옵션:

- `as_retriever`: 기본 문서 검색, 유사도 점수와 함께 `NodeWithScore` 리스트 반환
- `as_query_engine`: 단일 Q&A, 텍스트 응답 반환
- `as_chat_engine`: 대화형, 여러 메시지와 컨텍스트 유지하며 응답

에이전트적 상호작용에는 쿼리 엔진이 가장 일반적입니다.
응답에 사용할 LLM도 함께 전달합니다.

```python
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")
query_engine = index.as_query_engine(
    llm=llm,
    response_mode="tree_summarize",
)
query_engine.query("What is the meaning of life?")
# The meaning of life is 42
```

### 응답 처리

쿼리 엔진은 LLM뿐 아니라 `ResponseSynthesizer` 전략도 활용해 응답을 만듭니다.
기본 제공 전략은 세 가지:

- `refine`: 각 검색 결과를 순차적으로 LLM에 전달해 답변을 점진적으로 만듦(노드별 LLM 호출)
- `compact`(기본): 여러 조각을 미리 합쳐 LLM 호출 횟수를 줄임
- `tree_summarize`: 각 조각을 트리 구조로 요약해 상세 답변 생성

<Tip>쿼리 워크플로우를 세밀하게 제어하려면 <a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/usage_pattern/#low-level-composition-api">Low-level composition API</a> 참고</Tip>

LLM의 답변이 항상 정확하진 않으므로, **응답 품질을 평가**해야 합니다.

### 평가 및 관측

LlamaIndex는 **응답 품질 평가 도구**를 내장합니다.
LLM을 활용해 다양한 기준으로 응답을 분석합니다. 주요 평가자:

- `FaithfulnessEvaluator`: 답변이 컨텍스트에 근거하는지 평가
- `AnswerRelevancyEvaluator`: 답변이 질문과 관련 있는지 평가
- `CorrectnessEvaluator`: 답변이 실제로 맞는지 평가

<Tip>에이전트 관측/평가에 대해 더 배우려면 <a href="https://huggingface.co/learn/agents-course/bonus-unit2/introduction">Bonus Unit 2</a> 참고</Tip>

```python
from llama_index.core.evaluation import FaithfulnessEvaluator

query_engine = # 이전 섹션에서 생성
llm = # 이전 섹션에서 생성

# 인덱스 쿼리
evaluator = FaithfulnessEvaluator(llm=llm)
response = query_engine.query(
    "미국 혁명 당시 뉴욕시에서 어떤 전투가 있었나요?"
)
eval_result = evaluator.evaluate_response(response=response)
eval_result.passing
```

직접적인 평가 없이도, **관찰 가능성을 통해 우리 시스템이 어떻게 작동하고 있는지 통찰력을 얻을 수 있습니다.**
이는 특히 더 복잡한 워크플로우를 구축하고 각 컴포넌트가 어떻게 작동하는지 이해하고 싶을 때 유용합니다.

<details>
<summary>LlamaTrace 설치하기</summary>

<a href="./llama-hub">LlamaHub 섹션</a>에서 소개한 것처럼, 다음 명령어로 Arize Phoenix의 LlamaTrace 콜백을 설치할 수 있습니다:

```bash
pip install -U llama-index-callbacks-arize-phoenix
```

추가로, `PHOENIX_API_KEY` 환경 변수를 LlamaTrace API 키로 설정해야 합니다. 다음과 같이 얻을 수 있습니다:
- [LlamaTrace](https://llamatrace.com/login)에서 계정 생성
- 계정 설정에서 API 키 생성
- 아래 코드에서 API 키를 사용하여 추적 활성화

</details>

```python
import llama_index
import os

PHOENIX_API_KEY = "<PHOENIX_API_KEY>"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"api_key={PHOENIX_API_KEY}"
llama_index.core.set_global_handler(
    "arize_phoenix",
    endpoint="https://llamatrace.com/v1/traces"
)
```

<Tip>컴포넌트와 그 사용 방법에 대해 더 자세히 알고 싶으신가요? <a href="https://docs.llamaindex.ai/en/stable/module_guides/">컴포넌트 가이드</a> 또는 <a href="https://docs.llamaindex.ai/en/stable/understanding/rag/">RAG 가이드</a>와 함께 여정을 계속하세요.</Tip>

컴포넌트를 사용하여 `QueryEngine`을 만드는 방법을 살펴보았습니다. 이제 **`QueryEngine`을 에이전트의 도구로 사용하는 방법**을 알아보겠습니다! 
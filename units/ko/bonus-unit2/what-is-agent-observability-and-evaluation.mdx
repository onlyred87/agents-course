# AI 에이전트 관측성과 평가

## 🔎 관측성이란?

관측성(Observability)이란 로그, 지표, 트레이스 등 외부 신호를 통해 AI 에이전트 내부에서 무슨 일이 일어나는지 이해하는 것입니다. AI 에이전트의 경우, 행동, 도구 사용, 모델 호출, 응답 등을 추적해 디버깅과 성능 개선에 활용합니다.

![Observability dashboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## 🔭 왜 에이전트 관측성이 중요한가

관측성이 없으면 AI 에이전트는 "블랙박스"입니다. 관측성 도구는 에이전트를 투명하게 만들어 다음을 가능하게 합니다:

- 비용과 정확도 트레이드오프 이해
- 지연(latency) 측정
- 유해 언어 및 프롬프트 인젝션 탐지
- 사용자 피드백 모니터링

즉, 데모용 에이전트를 실제 서비스에 투입할 수 있게 해줍니다!

## 🔨 관측성 도구

AI 에이전트용 대표 관측성 도구로는 [Langfuse](https://langfuse.com), [Arize](https://www.arize.com) 등이 있습니다. 이 도구들은 상세 트레이스 수집, 실시간 대시보드 제공 등으로 문제 탐지와 성능 최적화를 쉽게 해줍니다.

관측성 도구는 기능과 특성이 매우 다양합니다. 일부는 오픈소스로 커뮤니티의 활발한 기여와 다양한 통합을 지원하고, 어떤 도구는 관측성, 평가, 프롬프트 관리 등 LLMOps의 특정 부분에 특화되어 있습니다. 여러 옵션의 문서를 참고해 자신에게 맞는 솔루션을 선택하세요.

[smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index) 등 많은 에이전트 프레임워크는 [OpenTelemetry](https://opentelemetry.io/docs/) 표준을 사용해 메타데이터를 관측성 도구에 노출합니다. 또한, 관측성 도구는 LLM 환경의 빠른 변화에 맞춰 커스텀 계측 기능도 제공합니다. 사용하는 도구의 문서를 꼭 확인하세요.

## 🔬 트레이스와 스팬

관측성 도구는 보통 에이전트 실행을 트레이스(trace)와 스팬(span)으로 표현합니다.

- **트레이스(trace):** 에이전트의 전체 작업(예: 사용자 쿼리 처리) 전체
- **스팬(span):** 트레이스 내 개별 단계(예: LLM 호출, 데이터 검색 등)

![Example of a smolagent trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## 📊 주요 모니터링 지표

관측성 도구가 주로 모니터링하는 대표 지표는 다음과 같습니다:

**지연(Latency):** 에이전트가 얼마나 빨리 응답하는가? 대기 시간이 길면 사용자 경험이 나빠집니다. 전체 작업과 개별 단계별 지연을 트레이싱해 측정하세요. 예를 들어, 모든 모델 호출에 20초가 걸린다면 더 빠른 모델 사용이나 병렬 호출로 개선할 수 있습니다.

**비용(Costs):** 에이전트 1회 실행에 드는 비용은? LLM 호출(토큰 단위 과금)이나 외부 API 사용이 많으면 비용이 빠르게 증가합니다. 품질 향상을 위해 LLM을 여러 번 호출한다면, 그 비용이 정당한지 평가해야 합니다. 실시간 모니터링으로 버그 등으로 인한 과도한 호출도 감지할 수 있습니다.

**요청 오류(Request Errors):** 에이전트가 실패한 요청 수는? API 오류, 도구 호출 실패 등이 포함됩니다. 운영 환경에서 더 견고하게 만들려면, 백업 모델 전환 등 폴백/재시도 로직을 추가하세요.

**사용자 피드백(User Feedback):** 직접적인 사용자 평가(👍/👎, 별점, 코멘트 등)는 매우 유용합니다. 부정적 피드백이 반복되면 에이전트가 기대대로 동작하지 않는 신호입니다.

**암묵적 사용자 피드백(Implicit Feedback):** 명시적 평가 없이도, 즉각적인 질문 재작성, 반복 질의, 재시도 버튼 클릭 등 사용자 행동이 간접 피드백이 됩니다. 같은 질문 반복은 에이전트가 기대대로 동작하지 않는 신호입니다.

**정확도(Accuracy):** 에이전트가 올바른 결과를 얼마나 자주 내는가? 정확도 정의는 문제 해결, 정보 검색, 사용자 만족 등 다양합니다. 성공 기준을 명확히 정의하고, 자동 체크, 평가 점수, 작업 완료 라벨 등으로 추적하세요. 예: 트레이스를 "성공"/"실패"로 표시.

**자동 평가 지표(Automated Evaluation Metrics):** LLM을 활용해 에이전트 출력을 평가할 수도 있습니다. [RAGAS](https://docs.ragas.io/) (RAG 에이전트용), [LLM Guard](https://llm-guard.com/) (유해 언어/프롬프트 인젝션 탐지) 등 오픈소스 라이브러리도 활용하세요.

실제로는 여러 지표를 조합해 에이전트의 상태를 종합적으로 파악하는 것이 가장 좋습니다. 이 챕터의 [예제 노트북](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb)에서 실제 예시를 보여드릴 예정입니다. 먼저, 전형적인 평가 워크플로우를 알아봅시다.

## 👍 AI 에이전트 평가

관측성은 지표를 제공하고, 평가는 그 데이터를 분석(및 테스트 수행)해 에이전트의 성능과 개선 방향을 결정하는 과정입니다. 즉, 트레이스와 지표를 바탕으로 에이전트를 평가하고 의사결정을 내립니다.

정기적인 평가는 중요합니다. AI 에이전트는 비결정적이고, 업데이트나 모델 변화로 동작이 달라질 수 있기 때문입니다. 평가 없이는 "스마트 에이전트"가 실제로 잘 동작하는지, 오히려 퇴보했는지 알 수 없습니다.

AI 에이전트 평가는 **온라인 평가**와 **오프라인 평가** 두 가지로 나뉩니다. 둘 다 중요하며 상호 보완적입니다. 보통 오프라인 평가부터 시작합니다.

### 🥷 오프라인 평가

![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

오프라인 평가는 실제 사용자 쿼리가 아닌, 테스트 데이터셋 등 통제된 환경에서 에이전트를 평가하는 것입니다. 정답이 명확한 데이터셋을 활용해 에이전트를 실행하고, 결과를 비교합니다.

예를 들어, 수학 문제 에이전트라면 [테스트 데이터셋](https://huggingface.co/datasets/gsm8k) 100문제의 정답을 알고 있고, 오프라인 평가는 개발 중(또는 CI/CD 파이프라인에서) 개선 여부나 퇴보 방지에 활용됩니다. 반복 가능하고, 정답이 있으므로 정확도 측정이 명확합니다. 이상에서 설명한 자동 평가 지표도 활용할 수 있습니다.

오프라인 평가의 핵심 과제는 테스트셋의 포괄성과 최신성 유지입니다. 고정된 테스트셋에서만 잘 동작하고 실제 환경에서는 다른 쿼리에 취약할 수 있으므로, 실제 사례와 엣지 케이스를 반영해 테스트셋을 지속적으로 보강해야 합니다. 소규모 "스모크 테스트"와 대규모 평가셋을 병행하면 좋습니다.

### 🔄 온라인 평가

온라인 평가는 실제 서비스 환경에서, 즉 실사용 중인 에이전트의 성능을 평가하는 것입니다. 실시간 사용자 상호작용을 모니터링하고 결과를 분석합니다.

예를 들어, 성공률, 사용자 만족도, 기타 지표를 실시간 트래픽에서 추적합니다. 온라인 평가는 **실험실 환경에서 예측하지 못한 상황까지 포착**할 수 있다는 장점이 있습니다. 예: 입력 패턴 변화로 모델 성능이 저하되는 "드리프트" 감지, 테스트셋에 없던 새로운 쿼리 등. 실제 환경에서의 에이전트 동작을 파악할 수 있습니다.

온라인 평가는 명시적/암묵적 사용자 피드백 수집, 섀도우 테스트, A/B 테스트(새 버전과 기존 버전 동시 운영) 등 다양한 방법을 포함합니다. 실시간 상호작용에 대한 신뢰도 높은 라벨/점수 확보가 어렵다는 점이 도전 과제입니다.

### 🤝 두 방법의 결합

실제로는 **온라인**과 **오프라인** 평가를 병행하는 것이 가장 효과적입니다. 오프라인 벤치마크로 정량적 성능을 점검하고, 실시간 모니터링으로 벤치마크에서 놓친 문제를 포착하세요. 예: 코드 생성 에이전트의 오프라인 성공률이 개선되는지 확인하고, 온라인에서는 새로운 유형의 질문에 취약한지 감지.

많은 팀이 다음과 같은 루프를 채택합니다: _오프라인 평가 → 새 버전 배포 → 온라인 지표 모니터링 및 실패 사례 수집 → 실패 사례를 오프라인 테스트셋에 추가 → 반복_. 이렇게 하면 평가가 지속적으로 개선됩니다.

## 🧑‍💻 실제 적용 예시

다음 섹션에서는 관측성 도구를 활용해 에이전트를 모니터링·평가하는 실제 예시를 살펴봅니다. 